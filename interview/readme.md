# 个人面经汇总

> 作者：杨夕
> 
> 项目地址：https://github.com/km1994/nlp_paper_study
> 
> 个人介绍：大佬们好，我叫杨夕，该项目主要是本人在研读顶会论文和复现经典论文过程中，所见、所思、所想、所闻，可能存在一些理解错误，希望大佬们多多指正。


## 一、基础算法篇

### 1. 二分查找递归 和 非递归的时间复杂度 和 空间复杂度

1. 递归
   
- 时间复杂度：O(logN)
- 空间复杂度：O(logN)
  
![](img/20200812204613.png)

2. 非递归

- 时间复杂度：O(logN)
- 空间复杂度：O(1)

### 2. 稳定和非稳定排序算法

<table>
    <tr>
        <td></td>
        <td>稳定</td>
        <td>非稳定</td>
    </tr>
    <tr>
        <td>特点</td>
        <td>通俗地讲就是能保证排序前2个相等的数其在序列的前后位置顺序和排序后它们两个的前后位置顺序相同。在简单形式化一下，如果Ai = Aj，Ai原来在位置前，排序后Ai还是要在Aj位置前。</td>
        <td>相反</td>
    </tr>
    <tr>
        <td>算法</td>
        <td>冒泡排序、插入排序、归并排序、基数排序</td>
        <td>选择排序、快速排序、希尔排序、堆排序</td>
    </tr>
</table>

## 二、机器算法篇

### 1. 如何判断过拟合，欠拟合，怎么防止过拟合，欠拟合？

(1) 欠拟合问题及解决方法

- 问题表现：高偏差
  - 当训练集和测试集的误差收敛但却很高时，为高偏差;
  - 当偏差很高，训练集和验证集的准确率都很低，很可能是欠拟合；
- 解决方法：增加模型参数：
  - 构建更多特征；
  - 减少正则项；
- 举例：小学生学习 高等数学

(2) 过拟合问题及解决方法

- 问题表现：高方差
  - 当训练集和测试集的误差之间有大的差距时，为高方差；
  - 当方差很高，训练集和验证集的准确率相差太多，应该是过拟合
- 解决方法：
  - 增加训练集；
  - 减低模型复杂度；
  - 增加正则项；
  - 通过特征选择减少特征数；
- 举例：应试能力很强，实际应用能力很差。擅长背诵知识，却不懂得灵活利用知识

### 2. 欠拟合 和 过拟合 的解决方法

#### 2.1. 解决 欠拟合 的方法

1. 增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间;
2. 尝试非线性模型，比如核SVM 、决策树、DNN等模型;
3. 如果有正则项可以较小正则项参数 $\lambda$;
4. Boosting ,Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等.

#### 2.2. 解决过拟合 的方法

1. 交叉检验，通过交叉检验得到较优的模型参数;
2. 特征选择，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间;
3. 正则化，常用的有 L1、L2 正则。而且 L1正则还可以自动进行特征选择;
4. 如果有正则项则可以考虑增大正则项参数 $\lambda$;
5. 数据集扩增，增加训练数据可以有限的避免过拟合;
6. Bagging ,将多个弱学习器Bagging 一下效果会好很多，比如随机森林等；
7. 早停策略。本质上是交叉验证策略，选择合适的训练次数，避免训练的网络过度拟合训练数据；
8. DropOut策略。所谓的Dropout指的是在用前向传播算法和反向传播算法训练DNN模型时，一批数据迭代时，随机的从全连接DNN网络中去掉一部分隐藏层的神经元。　在对训练集中的一批数据进行训练时，我们随机去掉一部分隐藏层的神经元，并用去掉隐藏层的神经元的网络来拟合我们的一批训练数据。使用基于dropout的正则化比基于bagging的正则化简单，这显而易见，当然天下没有免费的午餐，由于dropout会将原始数据分批迭代，因此原始数据集最好较大，否则模型可能会欠拟合。

### 3. 正则化

正则化方法是指**在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项**，一般有L1正则与L2正则等。

### 3.1 L0、L1、L2 介绍及区别？

(1) L0 

- 介绍：指向量中非 0 的元素的个数，希望参数中的大部分元素是 0，希望参数是稀疏的
- 缺点：难以优化
  
(2) L1 （稀疏规则算子 Lasso regularization）

- 介绍：指 向量中各元素绝对值之和，是 L0 正则项的最优凸近似；
- 优点：
  - 比 L0 容易优化求解，L0存在NP难问题，所以 使用 较多；
  - L1范数是L0范数的最优凸近似；
- 参数稀疏 的 优点：
  - 特征选择：通过将无用特征所对应的权重设为0，以去除无用特征；
  - 可解释性：因为无用特征的权重对应权重都为0，所以只需要介绍权重不为 0 的特征；

(3) L2 （岭回归 Ridge Regression 或者 权重衰减 Weight Decay）

- 介绍：指 向量各元素的平方和然后求平方根
- 作用：防止过拟合问题
- 优点：
  - 防止过拟合，提升模型泛化能力；
  - 有助于处理 condition number不好的情况下矩阵求逆很困难的问题

(4) L1 vs L2

<table>
    <tr>
        <td></td>
        <td>L1</td>
        <td>L2</td>
    </tr>
    <tr>
        <td>目标</td>
        <td>绝对值最小化</td>
        <td>平方值最小化</td>
    </tr>
    <tr>
        <td>下降速度</td>
        <td>以绝对值函数方式下降，较快</td>
        <td>以二次函数函数方式下降，较慢</td>
    </tr>
    <tr>
        <td>规则化的代价函数</td>
        <td>图 1 </td>
        <td>图 2 </td>
    </tr>
    <tr>
        <td>最优解是 w1 和 w2 的取值</td>
        <td>L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置就会产生稀疏性，例如图中的相交点就有w1=0，而更高维的时候（想象一下三维的L1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性 </td>
        <td>因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1-regularization 能产生稀疏性，而L2-regularization 不行的原因了 </td>
    </tr>
    <tr>
        <td>总结</td>
        <td>L1 会趋向于产生少量的特征，而其他的特征都是0 </td>
        <td>L2 会选择更多的特征，这些特征都会接近于0 </td>
    </tr>
    <tr>
        <td>特点</td>
        <td>Lasso在特征选择时候非常有用 </td>
        <td>Ridge就只是一种规则化而已 </td>
    </tr>
    <tr>
        <td>使用选择方面</td>
        <td>特征多，但是其作用的特征少的情况【自动选择特征】 </td>
        <td>特征中起作用的特征多的情况 </td>
    </tr>
    <tr>
        <td>分布类型</td>
        <td>拉普拉斯分布 </td>
        <td>高斯分布 </td>
    </tr>
</table>

![图片](img/20200812200223.png)
> 图 1

![图片](img/20200812200551.png)
> 图 2

![](img/20200812200740.png)
> 图 3
> 
> 将模型空间限制在w的一个L1-ball 中。为了便于可视化，我们考虑两维的情况，在(w1, w2)平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为C的一个 norm ball 。等高线与 norm ball 首次相交的地方就是最优解：

### 3.2 Dropout 介绍？

- 方式：Dropout 通过 修改ANN中隐藏层的神经元个数 来 防止过拟合
- 操作：训练时，随机删除一些隐藏层神经元，即让他们以一定概率不工作
  
![](img/20200812203815.png)

> s1：在训练开始时，随机得删除一些（可以设定为一半，也可以为1/3，1/4等）隐藏层神经元，即认为这些神经元不存在，同时保持输入层与输出层神经元的个数不变;
> s2：然后按照BP学习算法对ANN中的参数进行学习更新（虚线连接的单元不更新，因为认为这些神经元被临时删除了）。这样一次迭代更新便完成了。下一次迭代中，同样随机删除一些神经元，与上次不一样，做随机选择。这样一直进行瑕疵，直至训练结束。

- 原因：由于每一轮都相当于在一个 新的 子网络上训练。那么最终得到的模型便是 无数个 子网络 共同训练 的成果，效果自然会更好







