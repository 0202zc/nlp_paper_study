# Lattice CNNs for Matching Based Chinese Question Answering

## 摘要

Short text matching often faces the challenges that there are great word mismatch and expression diversity between the two texts, which would be further aggravated in languages like Chinese where there is no natural space to segment words explicitly. （短文本匹配经常面临两个文本之间存在巨大的词不匹配和表达差异的挑战，而在像中文这样的语言中，由于没有自然的空间来明确划分词，这种情况会进一步加剧。）

In this paper, we propose a novel lattice-based CNN model (LCNs) to utilize multi-granularity information inherent in the word lattice while maintaining a strong ability to deal with the introduced noisy information for matching based question-answering in Chinese. （在本文中，我们提出了一种新颖的 lattice-based 的CNN模型（LCN），该模型可以利用 word lattice 中固有的多粒度信息，同时保持强大的能力来处理引入的嘈杂信息以匹配基于中文的问题。） 

We conduct extensive experiments on both document-based question answering and knowledge-based question answering tasks, and experimental results show that the LCNs models can significantly outperform state-of-the-art matching models and strong baselines by taking advantage of better ability to distill rich but discriminative information from the word lattice input. （我们对基于文档的问答和基于知识的问答任务进行了广泛的实验，实验结果表明，LCNs模型可以利用更好的提炼能力，大大胜过最新的匹配模型和强大的基线 来自字格输入的丰富但有区别的信息。）


## 动机

However, matching text sequences for Chinese or similar languages often suffers from word segmentation, where there are often no perfect Chinese word segmentation tools that suit every scenario. Text matching usually requires to capture the relatedness between two sequences in multiple granularities. For example, in Figure 1, the example phrase is generally tokenized as “China – citizen – life – quality – high”, but when we plan to match it with “Chinese – live – well”, it would be more helpful to have the example segmented into “Chinese – livelihood – live” than its common segmentation.（但是，针对中文或类似语言的匹配文本序列经常会遭受分词的困扰，因为在这种情况下，通常没有完美的中文分词工具可以适合每种情况。 文本匹配通常需要捕获多个粒度的两个序列之间的相关性。 例如，在图1中，示例短语通常被标记为“中国–公民–生活–质量–高”，但是当我们计划将其与“中国人–生活–好”相匹配时，使用 例如，将其细分为“中国人-生计-生活”，而不是普通的细分。）

![](img/1.png)

Existing efforts use neural network models to improve the matching based on the fact that distributed representations can generalize discrete word features in traditional bag-of-words methods. And there are also works fusing word level and character-level information, which, to some extent, could relieve the mismatch between different segmentations, but these solutions still suffer from the original word sequential structures. They usually depend on an existing word tokenization, which has to make segmentation choices at one time, e.g., “ZhongGuo”(China) and “ZhongGuoRen”(Chinese) when processing “ZhongGuoRenMin”(Chinese people). And the blending just conducts at one position in their frameworks.(基于分布式表示可以概括传统词袋方法中离散词的特征，现有的工作使用神经网络模型来改进匹配。 并且还有融合词级和字符级信息的工作，在某种程度上可以缓解不同分段之间的不匹配，但是这些解决方案仍然受到原始词序结构的困扰。 它们通常依赖于现有的单词标记化，在处理“中国”时，必须一次做出切分选择，例如“中国”（中国）和“中国人民”（中文）。 混合只是在其框架中的一个位置进行。)

Specific tasks such as question answering (QA) could pose further challenges for short text matching. In document-based question answering (DBQA), the matching degree is expected to reflect how likely a sentence can answer a given question, where questions and candidate answer sentences usually come from different sources and may exhibit significantly different styles or syntactic structures, e.g. queries in web search and sentences in web pages. This could further aggravate the mismatch problems. In knowledge-based question answering (KBQA), one of the key tasks is to match relational expressions in questions with knowledge base (KB) predicate phrases3, such as “ZhuCeDi”(place of incorporation). Here the diversity between the two kinds of expressions is even more significant, where there may be dozens of different verbal expressions in natural language questions corresponding to only one KB predicate phrase. Those expression problems make KBQA a further tough task. Previous works (Yih, He, and Meek 2014; Yih et al. 2015) adopt letter-trigrams for diverse expressions, which is similar to the character level of Chinese. And the lattices are combinations of words and characters, so with lattices, we can utilize words information at the same time.（诸如问答（QA）之类的特定任务可能会对短文本匹配提出进一步的挑战。在基于文档的问题解答（DBQA）中，匹配度预计将反映句子回答给定问题的可能性，其中问题和候选答案句子通常来自不同的来源，并且可能表现出明显不同的样式或句法结构，例如网络搜索中的查询和网页中的句子。这可能会进一步加剧不匹配问题。在基于知识的问题解答（KBQA）中，关键任务之一是使问题中的关系表达与知识库（KB）谓词短语3相匹配，例如“ ZhuCeDi”（注册地）。在这里，两种表达方式之间的差异更为显着，在自然语言问题中可能存在数十种不同的语言表达方式，仅对应一个KB谓词短语。这些表达问题使KBQA变得更加艰巨。先前的作品（Yih，He和Meek，2014； Yih等，2015）采用字母三字表示形式来表达不同的文字，这与中文的字符水平相似。格子是单词和字符的组合，因此有了格子，我们可以同时利用单词信息。）


## 方法

In this paper, we propose a multi-granularity method for short text matching in Chinese question answering which utilizes lattice based CNNs to extract sentence level features over word lattice. Specifically, instead of relying on character or word level sequences, LCNs take word lattices as input, where every possible word and character will be treated equally and have their own context so that they can interact at every layer. For each word in each layer, LCNs can capture different context words in different granularity via pooling methods. To the best of our knowledge, we are the first to introduce word lattice into the text matching tasks. Because of the similar IO structures to original CNNs and the high efficiency, LCNs can be easily adapted to more scenarios where flexible sentence representation modeling is required.（在本文中，我们提出了一种用于中文问答中短文本匹配的多粒度方法，该方法利用基于格的CNN提取单词格上的句子级特征。 具体而言，LCN不再依赖于字符或单词级别序列，而是将单词格作为输入，其中每个可能的单词和字符将被平等对待并具有各自的上下文，以便它们可以在每一层进行交互。 对于每层中的每个单词，LCN可以通过合并方法以不同的粒度捕获不同的上下文单词。 据我们所知，我们是第一个将词格引入文本匹配任务的人。 由于与原始CNN相似的IO结构和高效率，LCN可以轻松地适应需要灵活的句子表示模型的更多方案。）


## 结论

In this paper, we propose a novel neural network matching method (LCNs) for matching based question-answering in Chinese. Rather than relying on a word sequence only, our model takes word lattice as input. By performing CNNs over multiple n-gram context to exploit multi-granularity information, LCNs can relieve the word mismatch challenges. Thorough experiments show that our model can better explore the word lattice via convolutional operations and rich context-aware pooling, thus outperforms the state-of-the-art models and competitive baselines by a large margin. Further analyses exhibit that lattice input takes advantage of the word and character-level information, and the vocabulary-based lattice constructor outperforms the strategies that combine characters and different word segmentation together. （在本文中，我们提出了一种新的基于神经网络的中文匹配问题的神经网络匹配方法（LCN）。 我们的模型不是仅依赖单词序列，而是将单词晶格作为输入。 通过在多个n-gram上下文上执行CNN以利用多粒度信息，LCN可以缓解单词不匹配的挑战。 全面的实验表明，我们的模型可以通过卷积运算和丰富的上下文感知池更好地探索单词晶格，因此在很大程度上优于最新模型和竞争基准。 进一步的分析表明，晶格输入利用了单词和字符级别的信息，而基于词汇的晶格构造器的性能优于将字符和不同的单词分割组合在一起的策略。） 

