{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\program\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\program\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\program\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\program\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\program\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\program\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\program\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\program\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\program\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\program\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\program\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import datetime\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import logging\n",
    "\n",
    "# 创建一个logger\n",
    "logger = logging.getLogger('mylogger')\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个handler，用于写入日志文件\n",
    "timestamp = str(int(time.time()))\n",
    "fh = logging.FileHandler('./log/log_' + timestamp +'.txt')\n",
    "fh.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再创建一个handler，用于输出到控制台\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# 定义handler的输出格式\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "# 给logger添加handler\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.embedding_dim = 100\n",
    "        self.num_filters_A = 50\n",
    "        self.num_filters_B = 50\n",
    "        self.n_hidden = 150\n",
    "        self.sentence_length = 100\n",
    "        self.num_classes = 6\n",
    "        self.l2_reg_lambda = 1 \n",
    "        self.num_epochs = 85\n",
    "        self.batch_size = 64\n",
    "        self.display_step = 100             # Evaluate model on dev set after this many steps (default: 100)\n",
    "        self.evaluate_every = 100 \n",
    "        self.checkpoint_every = 100         # Save model after this many steps (default: 100)\n",
    "        self.num_checkpoints = 5\n",
    "        self.lr = 1e-3\n",
    "        self.allow_soft_placement = True   # Allow device soft device placement\n",
    "        self.log_device_placement = False  # Log placement of ops on devices\n",
    "        self.filter_size = [1, 2, 100]\n",
    "        self.emb_size = 100 \n",
    "        self.train_path = './sts/semeval-sts/all'\n",
    "        self.test_path = './sts/semeval-sts/2016'\n",
    "        \n",
    "conf = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Embedder(object):\n",
    "    \"\"\" Generic embedding interface.\n",
    "\n",
    "    Required:\n",
    "      * w: dict mapping tokens to indices\n",
    "      * g: matrix with one row per token index\n",
    "      * N: embedding dimensionality\n",
    "    \"\"\"\n",
    "\n",
    "    def map_tokens(self, tokens, ndim=2):\n",
    "        \"\"\" for the given list of tokens, return a list of GloVe embeddings,\n",
    "        or a single plain bag-of-words average embedding if ndim=1.\n",
    "\n",
    "        Unseen words (that's actually *very* rare) are mapped to 0-vectors. \"\"\"\n",
    "        gtokens = [self.g[self.w[t]] for t in tokens if t in self.w]\n",
    "        if not gtokens:\n",
    "            return np.zeros((1, self.N)) if ndim == 2 else np.zeros(self.N)\n",
    "        gtokens = np.array(gtokens)\n",
    "        if ndim == 2:\n",
    "            return gtokens\n",
    "        else:\n",
    "            return gtokens.mean(axis=0)\n",
    "\n",
    "    def map_set(self, ss, ndim=2):\n",
    "        \"\"\" apply map_tokens on a whole set of sentences \"\"\"\n",
    "        return [self.map_tokens(s, ndim=ndim) for s in ss]\n",
    "\n",
    "    def map_jset(self, sj):\n",
    "        \"\"\" for a set of sentence emb indices, get per-token embeddings \"\"\"\n",
    "        return self.g[sj]\n",
    "\n",
    "    def pad_set(self, ss, spad, N=None):\n",
    "        \"\"\" Given a set of sentences transformed to per-word embeddings\n",
    "        (using glove.map_set()), convert them to a 3D matrix with fixed\n",
    "        sentence sizes - padded or trimmed to spad embeddings per sentence.\n",
    "\n",
    "        Output is a tensor of shape (len(ss), spad, N).\n",
    "\n",
    "        To determine spad, use something like\n",
    "            np.sort([np.shape(s) for s in s0], axis=0)[-1000]\n",
    "        so that typically everything fits, but you don't go to absurd lengths\n",
    "        to accomodate outliers.\n",
    "        \"\"\"\n",
    "        ss2 = []\n",
    "        if N is None:\n",
    "            N = self.N\n",
    "        for s in ss:\n",
    "            if spad > s.shape[0]:\n",
    "                if s.ndim == 2:\n",
    "                    s = np.vstack((s, np.zeros((spad - s.shape[0], N))))\n",
    "                else:  # pad non-embeddings (e.g. toklabels) too\n",
    "                    s = np.hstack((s, np.zeros(spad - s.shape[0])))\n",
    "            elif spad < s.shape[0]:\n",
    "                s = s[:spad]\n",
    "            ss2.append(s)\n",
    "        return np.array(ss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class GloVe(Embedder):\n",
    "    \"\"\" A GloVe dictionary and the associated N-dimensional vector space \"\"\"\n",
    "    def __init__(self, N=50, glovepath='D:/project/python_wp/nlp/data/glove.6B/glove.6B.%dd.txt'):\n",
    "        \"\"\" Load GloVe dictionary from the standard distributed text file.\n",
    "\n",
    "        Glovepath should contain %d, which is substituted for the embedding\n",
    "        dimension N. \"\"\"\n",
    "        self.N = N\n",
    "        self.w = dict()\n",
    "        self.g = []\n",
    "        self.glovepath = glovepath % (N,)\n",
    "\n",
    "        # [0] must be a zero vector\n",
    "        self.g.append(np.zeros(self.N))\n",
    "\n",
    "        with open(self.glovepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                l = line.split()\n",
    "                word = l[0]\n",
    "                self.w[word] = len(self.g)\n",
    "                self.g.append(np.array(l[1:]).astype(float))\n",
    "        self.w['UKNOW'] = len(self.g)\n",
    "        self.g.append(np.zeros(self.N))\n",
    "        self.g = np.array(self.g, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading glove...\n"
     ]
    }
   ],
   "source": [
    "#glove是载入的次向量。glove.d是单词索引字典<word, index>，glove.g是词向量矩阵<词个数,300>\n",
    "print('loading glove...')\n",
    "glove = GloVe(N=conf.emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载 训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_sts(dsfile, glove, conf, skip_unlabeled=True):\n",
    "    \"\"\" load a dataset in the sts tsv format \"\"\"\n",
    "    s0 = []\n",
    "    s1 = []\n",
    "    labels = []\n",
    "    with codecs.open(dsfile, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            label, s0x, s1x = line.split('\\t')\n",
    "            if label == '':\n",
    "                continue\n",
    "            else:\n",
    "                score_int = int(round(float(label)))\n",
    "                y = [0] * conf.num_classes\n",
    "                y[score_int] = 1\n",
    "                labels.append(np.array(y))\n",
    "            for i, ss in enumerate([s0x, s1x]):\n",
    "                words = word_tokenize(ss)\n",
    "                index = []\n",
    "                for word in words:\n",
    "                    word = word.lower()\n",
    "                    if word in glove.w:\n",
    "                        index.append(glove.w[word])\n",
    "                    else:\n",
    "                        index.append(glove.w['UKNOW'])\n",
    "                left = conf.sentence_length - len(words)\n",
    "                pad = [0]*left\n",
    "                index.extend(pad)\n",
    "                if i == 0:\n",
    "                    s0.append(np.array(index))\n",
    "                else:\n",
    "                    s1.append(np.array(index))\n",
    "            #s0.append(word_tokenize(s0x))\n",
    "            #s1.append(word_tokenize(s1x))\n",
    "#     print(len(s0))\n",
    "    return (s0, s1, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def concat_datasets(datasets):\n",
    "    \"\"\" Concatenate multiple loaded datasets into a single large one.\n",
    "\n",
    "    Example: s0, s1, lab = concat_datasets([load_sts(d) for glob.glob('data/sts/semeval-sts/all/201[0-4]*')]) \"\"\"\n",
    "    s0 = []\n",
    "    s1 = []\n",
    "    labels = []\n",
    "    for s0x, s1x, labelsx in datasets:\n",
    "        s0 += s0x\n",
    "        s1 += s1x\n",
    "        labels += labelsx\n",
    "    return (np.array(s0), np.array(s1), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_set(glove, path, conf):\n",
    "    files = []\n",
    "    for file in os.listdir(path):\n",
    "        if os.path.isfile(path + '/' + file):\n",
    "            files.append(path + '/' + file)\n",
    "#     print(f\"files:{files}\")\n",
    "    s0, s1, labels = concat_datasets([load_sts(d, glove,conf) for d in files])\n",
    "    #s0, s1, labels = np.array(s0), np.array(s1), np.array(labels)\n",
    "    print('(%s) Loaded dataset: %d' % (path, len(s0)))\n",
    "    #e0, e1, s0, s1, labels = load_embedded(glove, s0, s1, labels)\n",
    "    return ([s0, s1], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(./sts/semeval-sts/all) Loaded dataset: 22592\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "Xtrain, ytrain = load_set(glove, conf.train_path, conf)\n",
    "Xtrain[0], Xtrain[1], ytrain = shuffle(Xtrain[0], Xtrain[1], ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[  1629,   1592,   1135, ...,      0,      0,      0],\n",
       "         [400001,   4042,  24128, ...,      0,      0,      0],\n",
       "         [    40,    347,     13, ...,      0,      0,      0],\n",
       "         ...,\n",
       "         [     1,   2169,   2346, ...,      0,      0,      0],\n",
       "         [  2750,   2924,     46, ...,      0,      0,      0],\n",
       "         [103939,  37707,      2, ...,      0,      0,      0]]),\n",
       "  array([[  8102,     91,    211, ...,      0,      0,      0],\n",
       "         [  4042,  24128,   3442, ...,      0,      0,      0],\n",
       "         [     1,   2390,    332, ...,      0,      0,      0],\n",
       "         ...,\n",
       "         [     1,   2169,   2346, ...,      0,      0,      0],\n",
       "         [   143,      2,   2522, ...,      0,      0,      0],\n",
       "         [103939,  37707,      2, ...,      0,      0,      0]])],\n",
       " array([[0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 1],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 0]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain,ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(./sts/semeval-sts/2016) Loaded dataset: 1186\n"
     ]
    }
   ],
   "source": [
    "Xtest, ytest = load_set(glove, conf.test_path, conf)\n",
    "Xtest[0], Xtest[1], ytest = shuffle(Xtest[0], Xtest[1], ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_l1_distance(x, y):\n",
    "    with tf.name_scope('l1_distance'):\n",
    "        d = tf.reduce_sum(tf.abs(tf.subtract(x, y)), axis=1)\n",
    "        return d\n",
    "\n",
    "def compute_euclidean_distance(x, y):\n",
    "    with tf.name_scope('euclidean_distance'):\n",
    "        d = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(x, y)), axis=1))\n",
    "        return d\n",
    "\n",
    "def compute_pearson_distance(x, y):\n",
    "    with tf.name_scope(\"pearson\"):\n",
    "        mid1 = tf.reduce_mean(x * y, axis=1) - \\\n",
    "                    tf.reduce_mean(x, axis=1) * tf.reduce_mean(y, axis=1)\n",
    "        mid2 = tf.sqrt(tf.reduce_mean(tf.square(x), axis=1) - tf.square(tf.reduce_mean(x, axis=1))) * \\\n",
    "               tf.sqrt(tf.reduce_mean(tf.square(y), axis=1) - tf.square(tf.reduce_mean(y, axis=1)))\n",
    "        return mid1 / mid2\n",
    "\n",
    "def compute_cosine_distance(x, y):\n",
    "    with tf.name_scope('cosine_distance'):\n",
    "        x_norm = tf.sqrt(tf.reduce_sum(tf.square(x), axis=1))\n",
    "        y_norm = tf.sqrt(tf.reduce_sum(tf.square(y), axis=1))\n",
    "        x_y = tf.reduce_sum(tf.multiply(x, y), axis=1)\n",
    "        d = tf.divide(x_y, tf.multiply(x_norm, y_norm))\n",
    "        return d\n",
    "\n",
    "def comU1(x, y):\n",
    "    result = [compute_cosine_distance(x, y), compute_l1_distance(x, y)]\n",
    "    # result = [compute_euclidean_distance(x, y), compute_euclidean_distance(x, y), compute_euclidean_distance(x, y)]\n",
    "    return tf.stack(result, axis=1)\n",
    "\n",
    "\n",
    "def comU2(x, y):\n",
    "    # result = [compute_cosine_distance(x, y), compute_euclidean_distance(x, y)]\n",
    "    # return tf.stack(result, axis=1)\n",
    "    return tf.expand_dims(compute_cosine_distance(x, y), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "def init_weight(shape, name):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, mean=0, stddev=1.0), name=name)\n",
    "    return var\n",
    "\n",
    "class MPCNN_Layer():\n",
    "    def __init__(self, num_classes, embedding_size, filter_sizes, num_filters, n_hidden,\n",
    "                 input_x1, input_x2, input_y, dropout_keep_prob, l2_reg_lambda):\n",
    "        '''\n",
    "\n",
    "        :param sequence_length:\n",
    "        :param num_classes:\n",
    "        :param embedding_size:\n",
    "        :param filter_sizes:\n",
    "        :param num_filters:\n",
    "        '''\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self .num_filters = num_filters\n",
    "        self.num_classes = num_classes\n",
    "        self.poolings = [tf.reduce_max, tf.reduce_min, tf.reduce_mean]\n",
    "\n",
    "        self.input_x1 = input_x1\n",
    "        self.input_x2 = input_x2\n",
    "        self.input_y = input_y\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.W1 = [init_weight([filter_sizes[0], embedding_size, 1, num_filters[0]], \"W1_0\"),\n",
    "                   init_weight([filter_sizes[1], embedding_size, 1, num_filters[0]], \"W1_1\"),\n",
    "                   init_weight([filter_sizes[2], embedding_size, 1, num_filters[0]], \"W1_2\")]\n",
    "        self.b1 = [tf.Variable(tf.constant(0.1, shape=[num_filters[0]]), \"b1_0\"),\n",
    "                   tf.Variable(tf.constant(0.1, shape=[num_filters[0]]), \"b1_1\"),\n",
    "                   tf.Variable(tf.constant(0.1, shape=[num_filters[0]]), \"b1_2\")]\n",
    "\n",
    "        self.W2 = [init_weight([filter_sizes[0], embedding_size, 1, num_filters[1]], \"W2_0\"),\n",
    "                   init_weight([filter_sizes[1], embedding_size, 1, num_filters[1]], \"W2_1\")]\n",
    "        self.b2 = [tf.Variable(tf.constant(0.1, shape=[num_filters[1], embedding_size]), \"b2_0\"),\n",
    "                   tf.Variable(tf.constant(0.1, shape=[num_filters[1], embedding_size]), \"b2_1\")]\n",
    "        self.h = num_filters[0]*len(self.poolings)*2 + \\\n",
    "                 num_filters[1]*(len(self.poolings)-1)*(len(filter_sizes)-1)*3 + \\\n",
    "                 len(self.poolings)*len(filter_sizes)*len(filter_sizes)*3\n",
    "        self.Wh = tf.Variable(tf.random_normal([604, n_hidden], stddev=0.01), name='Wh')\n",
    "        self.bh = tf.Variable(tf.constant(0.1, shape=[n_hidden]), name=\"bh\")\n",
    "\n",
    "        self.Wo = tf.Variable(tf.random_normal([n_hidden, num_classes], stddev=0.01), name='Wo')\n",
    "        self.bo = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"bo\")\n",
    "\n",
    "\n",
    "    def attention(self):\n",
    "        sent1_unstack = tf.unstack(self.input_x1, axis=1)\n",
    "        sent2_unstack = tf.unstack(self.input_x2, axis=1)\n",
    "        D = []\n",
    "        for i in range(len(sent1_unstack)):\n",
    "            d = []\n",
    "            for j in range(len(sent2_unstack)):\n",
    "                dis = compute_cosine_distance(sent1_unstack[i], sent2_unstack[j])\n",
    "                #dis:[batch_size, 1(channels)]\n",
    "                d.append(dis)\n",
    "            D.append(d)\n",
    "            print(i)\n",
    "        D = tf.reshape(D, [-1, len(sent1_unstack), len(sent2_unstack), 1])\n",
    "        A = [tf.nn.softmax(tf.expand_dims(tf.reduce_sum(D, axis=i), 2)) for i in [2, 1]]\n",
    "        atten_embed = []\n",
    "        atten_embed.append(tf.concat([self.input_x1, A[0] * self.input_x1], 2))\n",
    "        atten_embed.append(tf.concat([self.input_x2, A[1] * self.input_x2], 2))\n",
    "        return atten_embed\n",
    "\n",
    "    def per_dim_conv_layer(self, x, w, b, pooling):\n",
    "        '''\n",
    "\n",
    "        :param input: [batch_size, sentence_length, embed_size, 1]\n",
    "        :param w: [ws, embedding_size, 1, num_filters]\n",
    "        :param b: [num_filters, embedding_size]\n",
    "        :param pooling:\n",
    "        :return:\n",
    "        '''\n",
    "        # unpcak the input in the dim of embed_dim\n",
    "        input_unstack = tf.unstack(x, axis=2)\n",
    "        w_unstack = tf.unstack(w, axis=1)\n",
    "        b_unstack = tf.unstack(b, axis=1)\n",
    "        convs = []\n",
    "        for i in range(x.get_shape()[2]):\n",
    "            conv = tf.nn.conv1d(input_unstack[i], w_unstack[i], stride=1, padding=\"VALID\")\n",
    "            conv = slim.batch_norm(inputs=conv, activation_fn=tf.nn.tanh, is_training=self.is_training)\n",
    "            convs.append(conv)\n",
    "        conv = tf.stack(convs, axis=2)\n",
    "        pool = pooling(conv, axis=1)\n",
    "\n",
    "        return pool\n",
    "\n",
    "    def bulit_block_A(self, x):\n",
    "        #bulid block A and cal the similarity according to algorithm 1\n",
    "        out = []\n",
    "        with tf.name_scope(\"bulid_block_A\"):\n",
    "            for pooling in self.poolings:\n",
    "                pools = []\n",
    "                for i, ws in enumerate(self.filter_sizes):\n",
    "                    with tf.name_scope(\"conv-pool-%s\" %ws):\n",
    "                        conv = tf.nn.conv2d(x, self.W1[i], strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "                        conv = slim.batch_norm(inputs=conv, activation_fn=tf.nn.tanh, is_training=self.is_training)\n",
    "                        pool = pooling(conv, axis=1)\n",
    "                    pools.append(pool)\n",
    "                out.append(pools)\n",
    "            return out\n",
    "\n",
    "    def bulid_block_B(self, x):\n",
    "        out = []\n",
    "        with tf.name_scope(\"bulid_block_B\"):\n",
    "            for pooling in self.poolings[:-1]:\n",
    "                pools = []\n",
    "                with tf.name_scope(\"conv-pool\"):\n",
    "                    for i, ws in enumerate(self.filter_sizes[:-1]):\n",
    "                        with tf.name_scope(\"per_conv-pool-%s\" % ws):\n",
    "                            pool = self.per_dim_conv_layer(x, self.W2[i], self.b2[i], pooling)\n",
    "                        pools.append(pool)\n",
    "                    out.append(pools)\n",
    "            return out\n",
    "\n",
    "\n",
    "    def similarity_sentence_layer(self):\n",
    "        # atten = self.attention() #[batch_size, length, 2*embedding, 1]\n",
    "        sent1 = self.bulit_block_A(self.input_x1)\n",
    "        sent2 = self.bulit_block_A(self.input_x2)\n",
    "        fea_h = []\n",
    "        with tf.name_scope(\"cal_dis_with_alg1\"):\n",
    "            for i in range(3):\n",
    "                regM1 = tf.concat(sent1[i], 1)\n",
    "                regM2 = tf.concat(sent2[i], 1)\n",
    "                for k in range(self.num_filters[0]):\n",
    "                    fea_h.append(comU2(regM1[:, :, k], regM2[:, :, k]))\n",
    "\n",
    "        #self.fea_h = fea_h\n",
    "\n",
    "        fea_a = []\n",
    "        with tf.name_scope(\"cal_dis_with_alg2_2-9\"):\n",
    "            for i in range(3):\n",
    "                for j in range(len(self.filter_sizes)):\n",
    "                    for k in range(len(self.filter_sizes)):\n",
    "                        fea_a.append(comU1(sent1[i][j][:, 0, :], sent2[i][k][:, 0, :]))\n",
    "        #\n",
    "        sent1 = self.bulid_block_B(self.input_x1)\n",
    "        sent2 = self.bulid_block_B(self.input_x2)\n",
    "\n",
    "        fea_b = []\n",
    "        with tf.name_scope(\"cal_dis_with_alg2_last\"):\n",
    "            for i in range(len(self.poolings)-1):\n",
    "                for j in range(len(self.filter_sizes)-1):\n",
    "                    for k in range(self.num_filters[1]):\n",
    "                        fea_b.append(comU1(sent1[i][j][:, :, k], sent2[i][j][:, :, k]))\n",
    "        #self.fea_b = fea_b\n",
    "        return tf.concat(fea_h + fea_a + fea_b, 1)\n",
    "\n",
    "\n",
    "    def similarity_measure_layer(self, is_training=True):\n",
    "        self.is_training = is_training\n",
    "        fea = self.similarity_sentence_layer()\n",
    "        self.h_drop = tf.nn.dropout(fea, self.dropout_keep_prob)\n",
    "        # fea_h.extend(fea_a)\n",
    "        # fea_h.extend(fea_b)\n",
    "        #print len(fea_h), fea_h\n",
    "        #fea = tf.concat(fea_h+fea_a+fea_b, 1)\n",
    "        #print fea.get_shape()\n",
    "        with tf.name_scope(\"full_connect_layer\"):\n",
    "            h = tf.nn.tanh(tf.matmul(fea, self.Wh) + self.bh)\n",
    "            # h = tf.nn.dropout(h, self.dropout_keep_prob)\n",
    "            self.scores = tf.matmul(h, self.Wo) + self.bo\n",
    "            self.output = tf.nn.softmax(self.scores)\n",
    "        #     return o\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        reg = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(1e-4), tf.trainable_variables())\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # self.loss = -tf.reduce_sum(self.input_y * tf.log(self.output))\n",
    "            self.loss = tf.reduce_sum(tf.square(tf.subtract(self.input_y, self.output))) + reg\n",
    "\n",
    "            # self.loss = tf.reduce_mean(\n",
    "            #     tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y))\n",
    "            # self.loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.input_y, 1), tf.argmax(self.scores, 1)), tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x1_batch, x2_batch, y_batch):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "      input_1: x1_batch,\n",
    "      input_2: x2_batch,\n",
    "      input_3: y_batch,\n",
    "      dropout_keep_prob: 0.5\n",
    "    }\n",
    "    _, step, summaries, batch_loss, accuracy = sess.run(\n",
    "        [train_step, global_step, train_summary_op, setence_model.loss, setence_model.accuracy],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    logger.info(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, batch_loss, accuracy))\n",
    "    train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "def dev_step(x1_batch, x2_batch, y_batch, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a dev set\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "      input_1: x1_batch,\n",
    "      input_2: x2_batch,\n",
    "      input_3: y_batch,\n",
    "      dropout_keep_prob: 1\n",
    "    }\n",
    "    _, step, summaries, batch_loss, accuracy = sess.run(\n",
    "        [train_step, global_step, dev_summary_op, setence_model.loss, setence_model.accuracy],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    dev_summary_writer.add_summary(summaries, step)\n",
    "    # if writer:\n",
    "    #     writer.add_summary(summaries, step)\n",
    "\n",
    "    return batch_loss, accuracy\n",
    "\n",
    "\n",
    "def get_embedding():\n",
    "    gfile_path = os.path.join(\"./glove.6B\", \"glove.6B.300d.txt\")\n",
    "    f = open(gfile_path, 'r')\n",
    "    embeddings = {}\n",
    "    for line in f:\n",
    "        sp_value = line.split()\n",
    "        word = sp_value[0]\n",
    "        embedding = [float(value) for value in sp_value[1:]]\n",
    "        embeddings[word] = embedding\n",
    "    print(\"read word2vec finished!\")\n",
    "    f.close()\n",
    "    return embeddings\n",
    "\n",
    "#load_sts('./sts/semeval-sts/2016/answer-answer.test.tsv')\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-730cfad91d52>:157: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "    # sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n",
    "    input_1 = tf.placeholder(tf.int32, [None, conf.sentence_length], name=\"input_x1\")\n",
    "    input_2 = tf.placeholder(tf.int32, [None, conf.sentence_length], name=\"input_x2\")\n",
    "    input_3 = tf.placeholder(tf.float32, [None, conf.num_classes], name=\"input_y\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "    with tf.name_scope(\"embendding\"):\n",
    "        s0_embed = tf.nn.embedding_lookup(glove.g, input_1)\n",
    "        s1_embed = tf.nn.embedding_lookup(glove.g, input_2)\n",
    "\n",
    "    with tf.name_scope(\"reshape\"):\n",
    "        input_x1 = tf.reshape(s0_embed, [-1, conf.sentence_length, conf.embedding_dim, 1])\n",
    "        input_x2 = tf.reshape(s1_embed, [-1, conf.sentence_length, conf.embedding_dim, 1])\n",
    "        input_y = tf.reshape(input_3, [-1, conf.num_classes])\n",
    "\n",
    "    # sent1_unstack = tf.unstack(input_x1, axis=1)\n",
    "    # sent2_unstack = tf.unstack(input_x2, axis=1)\n",
    "    # D = []\n",
    "    # for i in range(len(sent1_unstack)):\n",
    "    #     d = []\n",
    "    #     for j in range(len(sent2_unstack)):\n",
    "    #         dis = compute_cosine_distance(sent1_unstack[i], sent2_unstack[j])\n",
    "    #         d.append(dis)\n",
    "    #     D.append(d)\n",
    "    # D = tf.reshape(D, [-1, len(sent1_unstack), len(sent2_unstack), 1])\n",
    "    # A = [tf.nn.softmax(tf.expand_dims(tf.reduce_sum(D, axis=i), 2)) for i in [2, 1]]\n",
    "    #\n",
    "    # print A[1]\n",
    "    # print A[1] * input_x2\n",
    "    # atten_embed = tf.concat([input_x2, A[1] * input_x2], 2)\n",
    "\n",
    "    setence_model = MPCNN_Layer(conf.num_classes, conf.embedding_dim, conf.filter_size,\n",
    "                                [conf.num_filters_A, conf.num_filters_B], conf.n_hidden,\n",
    "                                input_x1, input_x2, input_y, dropout_keep_prob, conf.l2_reg_lambda)\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    setence_model.similarity_measure_layer()\n",
    "    optimizer = tf.train.AdamOptimizer(conf.lr)\n",
    "    grads_and_vars = optimizer.compute_gradients(setence_model.loss)\n",
    "    train_step = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    # print(\"Writing to {}\\n\".format(out_dir))\n",
    "    #\n",
    "    loss_summary = tf.summary.scalar(\"loss\", setence_model.loss)\n",
    "    acc_summary = tf.summary.scalar(\"accuracy\", setence_model.accuracy)\n",
    "    #\n",
    "    train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "    #\n",
    "    dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "    dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "    #\n",
    "    # checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    # checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    # if not os.path.exists(checkpoint_dir):\n",
    "    #     os.makedirs(checkpoint_dir)\n",
    "    # saver = tf.train.Saver(tf.global_variables(), max_to_keep=conf.num_checkpoints)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batches = batch_iter(list(zip(Xtrain[0], Xtrain[1], ytrain)), conf.batch_size, conf.num_epochs)\n",
    "    for batch in batches:\n",
    "        x1_batch, x2_batch, y_batch = zip(*batch)\n",
    "        train(x1_batch, x2_batch, y_batch)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % conf.evaluate_every == 0:\n",
    "            total_dev_loss = 0.0\n",
    "            total_dev_accuracy = 0.0\n",
    "\n",
    "            logger.info(\"\\nEvaluation:\")\n",
    "            dev_batches = batch_iter(list(zip(Xtest[0], Xtest[1], ytest)), conf.batch_size, 1)\n",
    "            for dev_batch in dev_batches:\n",
    "                x1_dev_batch, x2_dev_batch, y_dev_batch = zip(*dev_batch)\n",
    "                dev_loss, dev_accuracy = dev_step(x1_dev_batch, x2_dev_batch, y_dev_batch)\n",
    "                total_dev_loss += dev_loss\n",
    "                total_dev_accuracy += dev_accuracy\n",
    "            total_dev_accuracy = total_dev_accuracy / (len(ytest) / conf.batch_size)\n",
    "            logger.info(\"dev_loss {:g}, dev_acc {:g}, num_dev_batches {:g}\".format(total_dev_loss, total_dev_accuracy,\n",
    "                                                                             len(ytest) / conf.batch_size))\n",
    "            # train_summary_writer.add_summary(summaries)\n",
    "\n",
    "    #sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "    # for i in range(conf.num_epochs):\n",
    "    #     training_batch = zip(range(0, len(Xtrain[0]), conf.batch_size),\n",
    "    #                          range(conf.batch_size, len(Xtrain[0]) + 1, conf.batch_size))\n",
    "    #     for start, end in training_batch:\n",
    "    #         feed_dict = {input_1: Xtrain[0][start:end], input_2: Xtrain[1][start:end],\n",
    "    #                      dropout_keep_prob: 0.5, input_3: ytrain[start:end]}\n",
    "    #         print start\n",
    "    #         #assert all(x.shape == (100, 100) for x in Xtrain[0][start:end])\n",
    "    #         loss, _ = sess.run(train_step, feed_dict=feed_dict)\n",
    "    #         print(\"Epoch:\", '%04d' % (i + 1), \"cost=\", \"{:.9f}\".format(loss))\n",
    "\n",
    "    logger.info(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
