# 【关于 强化学习】 那些你不知道的事

> 作者：杨夕
> 
> 项目地址：https://github.com/km1994/nlp_paper_study
> 
> 个人介绍：大佬们好，我叫杨夕，该项目主要是本人在研读顶会论文和复现经典论文过程中，所见、所思、所想、所闻，可能存在一些理解错误，希望大佬们多多指正。

## 介绍

- 目标：一个智能体(agent)怎么在一个复杂不确定的环境(environment)里面去极大化它能获得的奖励；

![](img/微信截图_20201019080523.png)

> agent 和 environment。在强化学习过程中，agent 跟 environment 一直在交互。<br/>
> Agent 在环境里面获取到状态，agent 会利用这个状态输出一个 action，一个决策，然后这个决策会放到环境之中去。【目标：为了尽可能多地从环境中获取奖励】
> environment 环境会通过这个 agent 采取的决策，输出下一个状态以及当前的这个决策得到的奖励。

## supervised learning VS Reinforcement Learning

1. 首先强化学习输入的序列的数据并不是像 supervised learning 里面这些样本都是**独立**的。
2. 另外一点是 learner 并没有被告诉你每一步正确的行为应该是什么。Learner 不得不自己去发现哪些行为可以使得它最后得到这个奖励，只能通过不停地尝试来发现最有利的 action。【未知型】
3. 这里还有一点是 agent 获得自己能力的过程中，其实是通过不断地试错(trial-and-error exploration)。Exploration 和 exploitation 是强化学习里面非常核心的一个问题。Exploration 是说你会去尝试一些新的行为，这些新的行为有可能会使你得到更高的奖励，也有可能使你一无所有。Exploitation 说的是你就是就采取你已知的可以获得最大奖励的行为，你就重复执行这个 action 就可以了，因为你已经知道可以获得一定的奖励。因此，我们需要在 exploration 和 exploitation 之间取得一个权衡，这也是在监督学习里面没有的情况。
4. 在强化学习过程中，没有非常强的 supervisor，只有一个奖励信号(reward signal) ，就是环境会在很久以后告诉你之前你采取的行为到底是不是有效的。Agent在这个强化学习里面学习的话就非常困难，因为你没有得到即时反馈。当你采取一个行为过后，如果是监督学习，你就立刻可以获得一个指引，就说你现在做出了一个错误的决定，那么正确的决定应该是谁。而在强化学习里面，环境可能会告诉你这个行为是错误的，但是它并没有告诉你正确的行为是什么。而且更困难的是，它可能是在一两分钟过后告诉你错误，它再告诉你之前的行为到底行不行。所以这也是强化学习和监督学习不同的地方。

## Reinforcement Learning 特征

1. 首先它是有这个 trial-and-error exploration ，它需要通过探索环境来获取对这个环境的理解。
2. 第二点是强化学习 agent 会从环境里面获得延迟的奖励。
3. 第三点是在强化学习的训练过程中，时间非常重要。因为你得到的数据都是有这个时间关联的，而不是这个 i.i.d 分布的。在机器学习中，如果观测数据有非常强的关联，其实会使得这个训练非常不稳定。这也是为什么在监督学习中，我们希望 data 尽量是 i.i.d 了，这样就可以消除数据之间的相关性。
4. 第四点是这个 agent 的行为会影响它随后得到的数据，这一点是非常重要的。在我们训练 agent 的过程中，很多时候我们也是通过正在学习的这个 agent 去跟环境交互来得到数据。所以如果在训练过程中，这个 agent 的模型很快死掉了，那会使得我们采集到的数据是非常糟糕的，这样整个训练过程就失败了。所以在强化学习里面一个非常重要的【问题】就是**怎么让这个 agent 的行为一直稳定地提升**。

