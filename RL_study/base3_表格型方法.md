# 【关于 表格型方法】 那些你不知道的事

> 作者：杨夕
> 
> 项目地址：https://github.com/km1994/nlp_paper_study
> 
> 个人介绍：大佬们好，我叫杨夕，该项目主要是本人在研读顶会论文和复现经典论文过程中，所见、所思、所想、所闻，可能存在一些理解错误，希望大佬们多多指正。

1. P函数和R函数：P函数就是状态转移的概率，其就是反应的环境的随机性，R函数就是Reward function。但是我们通常处于一个未知的环境（即P函数和R函数是未知的）。
2. Q表格型表示方法： 表示形式是一种表格形式，其中横坐标为action（agent）的行为，纵坐标是环境的state，其对应着每一个时刻agent和环境的情况，并通过对应的reward反馈去做选择。一般情况下，Q表格是一个已经训练好的表格，不过，我们也可以每进行一步，就更新一下Q表格，然后用下一个状态的Q值来更新这个状态的Q值（即时序差分方法）。
3. 时序差分（Temporal Difference）： 一种Q函数（Q值）的更新方式，也就是可以拿下一步的 Q 值 Q(S_{t+1},A_{t+1}) 来更新我这一步的 Q 值 Q(S_{t},A_{t})。完整的算公式如下：

![](img/微信截图_20201102083235.png)

4. SARSA算法： 一种更新前一时刻状态的单步更新的强化学习算法，也是一种on-policy策略。该算法由于每次更新值函数需要知道前一步的状态(state)，前一步的动作(action)、奖励(reward)、当前状态(state)、将要执行的动作(action)，即 (S_{t},A_{t},R_{t+1},S_{t},A_{t}) 这几个值，所以被称为SARSA算法。agent没进行一次循环，都会用 (S_{t},A_{t},R_{t+1},S_{t},A_{t})  对于前一步的Q值（函数）进行一次更新。